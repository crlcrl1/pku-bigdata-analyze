\documentclass{article}

\usepackage{neurips_2022}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[UTF8, fontset=adobe]{ctex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{resizegather}
\usepackage{graphicx}
\usepackage{subfigure}

\hypersetup{
    colorlinks=true,
    linkbordercolor=white
}
\setlength{\parindent}{2em}

\title{大数据分析中的算法上机作业报告}

\author{陈润璘 2200010848}

\begin{document}

\maketitle

\section{问题描述}

\subsection{LASSO 问题}

LASSO 问题是一个经典的稀疏优化问题, 它由求满足一个线性方程组的稀疏解的 0-范数优化问题
\begin{equation*}
    \min_{x \in \mathbb{R}^n}\quad \frac{1}{2} \|Ax - b\|_2^2 + \mu \|x\|_0
\end{equation*}
松弛得到，其形式为
\begin{equation*}
    \min_{x \in \mathbb{R}^n}\quad \frac{1}{2} \|Ax - b\|_2^2 + \mu \|x\|_1
\end{equation*}

\subsection{低秩矩阵恢复问题}

低秩矩阵恢复问题是一个在推荐系统中的常见问题，它旨在恢复一个低秩矩阵中缺失的数据，其形式为
\begin{equation*}
    \min_{X \in \mathbb{R}^{m\times n}}\quad \sum_{(i,j)\in \Omega} (X_{ij} - M_{ij})^2 + \mu \|X\|_*
\end{equation*}
其中 $\Omega$ 是一个索引集合, $\|X\|_*$ 是 $X$ 的核范数.

\section{算法实现}

\subsection{LASSO 问题}

\subsubsection{使用 Mosek 和 Gurobi 求解}

\href{https://www.mosek.com/}{Mosek} 和 \href{https://www.gurobi.com/}{Gurobi} 是两个商业的优化求解器, 他们都提供了 Python 接口. 在本次作业中, 我们使用这两个求解器求解 LASSO 问题.

为了使用求解器求解 LASSO 问题, 我们需要将 LASSO 问题中的 1-范数转换为求解器可以接受的形式. 具体地, 我们可以使用 LASSO 问题的如下等价形式:
\begin{align*}
    \min_{x \in \mathbb{R}^n, t \in \mathbb{R}^n}\quad & \frac{1}{2} \|Ax - b\|_2^2 + \mu \sum_{i=1}^n t_i \\
    \text{s.t.}\quad & t_i \geq x_i, \quad i=1,\cdots,n\\
    & t_i \geq -x_i, \quad i=1,\cdots,n\\
    & t_i \geq 0, \quad i=1,\cdots,n
\end{align*}
这样可以去掉 1-范数中的绝对值, 使得求解器可以直接求解.

\subsubsection{近似点梯度法}

由于问题中的目标函数由一个可微函数和一个不可微函数组成, 因此我们可以使用近似点梯度法进行求解. 近似点梯度法的基本思想是在每次迭代中, 先使用梯度法更新可微的部分, 再应用\textbf{邻近算子}更新不可微的部分. 具体地, 在本问题中, 我们可以使用如下的迭代公式:
\begin{equation*}
    x_{k+1} = \text{prox}_{t_k\mu\|\cdot\|_1}(x_k - t_k \cdot \nabla f(x_k))
\end{equation*}
其中用邻近算子 $\text{prox}_h(x)$ 的定义如下:
\begin{equation*}
    \text{prox}_h(x) = \arg\min_y \left\{\frac{1}{2}\|y - x\|_2^2 + h(y)
    \right\}
\end{equation*}
在这个问题中, 我们可以计算 $\text{prox}_{t\|\cdot\|_1}(x)$ 的解析解:
\begin{equation*}
    \text{prox}_{t\|\cdot\|_{1}}(x)_i =
    \begin{cases}
        x_i - t & \text{if } x_i > t\\
        x_i + t & \text{if } x_i < -t\\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

\paragraph{步长的选取}

在近似点梯度法中, 步长的选取在很大程度上影响了算法的收敛速度. 在本次作业中, 我们使用了 BB 步长, 其核心思想是借鉴拟牛顿法中对 Hessian 矩阵的近似, 通过历史梯度和变量的变化来估计一个有效的步长, 而无需显式地计算 Hessian 矩阵. 具体地, 我们使用如下的公式来计算步长:
\begin{equation*}
    t_{k} = \frac{s_k^T s_k}{s_k^T y_k}
\end{equation*}
其中 $s_k = x_{k+1} - x_k$ 是变量变化, $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$ 是梯度变化.

\paragraph{重启策略}

由于当 $\mu$ 较小时, 直接使用近似点梯度法求解 LASSO 问题会导致算法收敛速度较慢, 因此我们使用了重启策略. 具体地, 我们从一个较大的 $\mu$ 开始, 然后逐渐减小 $\mu$, 每一次改变 $\mu$ 时都以上一次迭代的结果作为初始值, 从而逐步地靠近最优解.

\subsubsection{交替方向乘子法}

交替方法乘子法 (ADMM) 是一种求高效解优化问题的方法, 它通过将原问题分解为若干个子问题, 并通过交替求解这些子问题来求解原问题. 具体地, 对于优化问题
\begin{equation*}
    \begin{aligned}
        \min_{x_1, x_2}\quad & f_1(x_1) + f_2(x_2)\\
        \text{s.t.}\quad & A_1 x_1 + A_2 x_2 = b
    \end{aligned}
\end{equation*}
构造增广拉格朗日函数:
\begin{equation*}
    L_\rho(x_1, x_2; \lambda) = f_1(x_1) + f_2(x_2) + \left<\lambda, A_1 x_1 + A_2 x_2 - b\right> + \frac{\rho}{2}\|A_1 x_1 + A_2 x_2 - b\|_2^2
\end{equation*}
其中 $\lambda$ 是拉格朗日乘子, $\rho$ 是惩罚因子. 交替方向乘子法的迭代格式如下:
\begin{algorithm}
    \caption{交替方向乘子法}
    \begin{algorithmic}
        \State $x_1 \gets x_{1,0}$, $x_2 \gets x_{2,0}$, $\lambda \gets \lambda_0$ \Comment 初始化变量和乘子
        \While{not converge}
        \State $x_1 \gets \arg \min_{x_1} L_\rho(x_1,x_2;\lambda)$ \Comment 更新 $x_1$
        \State $x_2 \gets \arg \min_{x_2} L_\rho(x_1,x_2;\lambda)
        $ \Comment 更新 $x_2$
        \State $\lambda \gets \lambda + \tau(A_1 x_1 + A_2 x_2 - b)$ \Comment 更新乘子
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\newline
其中 $\tau\in \left(0,\frac{1+\sqrt{5}}{2}\right]$ 是步长.

\paragraph{原问题的交替方向乘子法}

对于 LASSO 问题, 可以按照如下的形式进行拆分:
\begin{equation*}
    \begin{aligned}
        \min_{x \in \mathbb{R}^{n\times l}}\quad &\frac{1}{2} \|Ax - b\|_2^2 +
        \mu \|y\|_1\\
        \text{s.t.}\quad & x=y
    \end{aligned}
\end{equation*}
然后构造增广拉格朗日函数:
\begin{equation*}
    L_\rho(x, y; \lambda) = \frac{1}{2} \|Ax - b\|_2^2 + \mu \|y\|_1 + \left<\lambda, x - y\right> + \frac{\rho}{2}\|x - y\|_2^2
\end{equation*}
使用交替方向乘子法求解. 其迭代格式如下:
\begin{align*}
    x_{k+1} &= \arg \min_{x} L_\rho(x, y_k; \lambda_k)\\
    & = \arg \min_{x} \frac{1}{2} \|Ax - b\|_2^2 + \left<\lambda_k, x\right> + \frac{\rho}{2}\|x - y_k\|_2^2\\
    y_{k+1} &= \arg \min_{y} L_\rho(x_{k+1}, y; \lambda_k)\\
    & = \arg \min_{y} \mu \|y\|_1 + \left<\lambda_k, -y\right> + \frac{\rho}{2}\|x_{k+1} - y\|_2^2\\
    & = \text{prox}_{\frac{\mu}{\rho}\|\cdot\|_1}(x_{k+1} + \frac{\lambda_k}{\rho})\\
    \lambda_{k+1} &= \lambda_k + \tau(x_{k+1} - y_{k+1})
\end{align*}

\paragraph{对偶问题的交替方向乘子法}

同样我们也可以对 LASSO 问题的对偶问题使用交替方向乘子法进行求解. LASSO 问题的对偶问题为
\begin{equation*}
    \begin{aligned}
        \min_{y\in \mathbb{R}^m}\quad &\left<b, y\right> + \frac{1}{2} \|y\|_2^2
        \\
        \text{s.t.}\quad & \|A^T y\|_{\infty} \leq \mu
    \end{aligned}
\end{equation*}
引入拆分, 把对偶问题改写为
\begin{equation*}
    \begin{aligned}
        \min_{y, z}\quad &\left<b, y\right> + \frac{1}{2} \|y\|_2^2
        \\
        \text{s.t.}\quad & z = A^T y\\
        & \|z\|_{\infty} \leq \mu
    \end{aligned}
\end{equation*}
其增广拉格朗日函数为
\begin{equation*}
    L_\rho(y, z; \lambda) = \left<b, y\right> + \frac{1}{2} \|y\|_2^2 + \left<\lambda, z - A^T y\right> + \frac{\rho}{2}\|z - A^T y\|_2^2
\end{equation*}
使用交替方向乘子法求解. 其迭代格式如下:
\begin{align*}
    y_{k+1} &= \arg \min_{y} L_\rho(y, z_k; \lambda_k)\\
    & = \arg \min_{y} \left<b-A\lambda_k, y\right> + \frac{1}{2} \|y\|_2^2  + \frac{\rho}{2}\|z_k - A^T y\|_2^2\\
    z_{k+1} &= \arg \min_{z} L_\rho(y_{k+1}, z; \lambda_k)\\
    & = \arg \min_{z} \frac{1}{2}\|z - A^T y_{k+1} + \frac{\lambda_k}{\rho}\|_2^2 + I_{\|\cdot\|_{\infty} \leq \mu}(z)\\
    & = P_{\|\cdot\|_{\infty} \leq \mu}(A^T y_{k+1} + \frac{\lambda_k}{\rho})\\
    \lambda_{k+1} &= \lambda_k - \tau(z_{k+1} - A^T y_{k+1})
\end{align*}
求解对偶问题后, 乘子 $\lambda$ 的值就是原问题的解.

\subsubsection{原始-对偶混合梯度法}

首先, 我们将 LASSO 问题写成鞍点形式:
\begin{equation*}
    \min_{x \in \mathbb{R}^n}\quad \max_{z \in \mathbb{R}^m}\quad f(x) - h^*(z) + \left<Ax, z\right>
\end{equation*}
其中 $h^*(z) = \frac{1}{2}\|z\|_2^2 + b^T z$. 应用PDHG算法, $x$, $z$ 的更新公式为:
\begin{align*}
    z_{k+1} &=\arg \max_{z} - h^*(z) + \left<Ax^k, z\right>\\
    &= \frac{1}{1+\delta_k}(z^k + \delta_k A x^k - \delta^k b)\\
    x_{k+1} &= \arg \min_x f(x) + \left<Ax, z^{k+1}\right>\\
    &= \text{prox}_{\alpha_k\mu\|\cdot\|_1}(x^k - \alpha_k A^T z_{k+1})\\
\end{align*}

与近似点梯度法类似, 直接对较小的 $\mu$ 求解 LASSO 问题较为困难, 因此我们同样采取重启策略, 从而较快地收敛到最优解.

\subsubsection{GPU 加速求解}

在使用迭代法求解 LASSO 问题时, 需要用到大量的矩阵运算, 因此我们可以使用现代 GPU 的通用计算能力来加速求解. 在本次作业中, 我们使用了 \href{https://pytorch.org/}{PyTorch} 来实现 GPU 加速.

在近似点梯度法中, 每一步迭代的主要计算量来源于计算梯度时的矩阵乘法. 在使用 ADMM 求解 LASSO 问题, 每一步迭代的主要计算量来源于计算一个矩阵的逆, 但是矩阵求逆无法很好地在 GPU 上加速, 因此使用 GPU 并不能显著加速 ADMM 求解 LASSO 问题. 与近似点梯度法类似, PDHG 算法中每一步迭代的主要计算量也来源于矩阵乘法, 因此使用 GPU 可以显著加速 PDHG 算法.

\subsubsection{算法展开}

对于近似点梯度法, 我们可以使用算法展开将算法的每一次迭代修改为如下的形式:
\begin{equation*}
    \begin{aligned}
        \text{grad} &\gets W_1^k x + W_2^k b\\
        x &\gets x - \eta_k\text{grad}\\
        x &\gets \text{prox}_{\eta_k\mu\|\cdot\|_1}(x)
    \end{aligned}
\end{equation*}
其中 $W_1$, $W_2$ 和 $\eta_k$ 是可学习的参数. 将上述的迭代多次叠加, 即可得到一个由近似点梯度法得到的神经网络.

记上述的网络为 $\text{net}(b)$ 在训练神经网络时, 可以使用以下的损失函数:
\begin{equation*}
    \mathcal{L}(x) = \sum_{i=0}^{N} \|\text{net}(b_i) - x_i^*\|_2^2
\end{equation*}
其中 $x_i^*$ 是第 $i$ 个样本的最优解.

\subsection{低秩矩阵恢复问题}

\subsubsection{近似点梯度法}

与 LASSO 问题类似, 低秩矩阵恢复问题也可以使用近似点梯度法进行求解. 具体地, 我们可以使用如下的迭代公式:
\begin{equation*}
    \begin{aligned}
        X_{k+1} &= \text{prox}_{t_k\mu\|\cdot\|_*}(X_k - t_k \cdot \nabla (\mathcal{A}(X_k)-b)^2)\\
        &= \text{prox}_{t_k\mu\|\cdot\|_*}(X_k - 2 t_k \mathcal{A}^*(\mathcal{A}(X_k)-b))
    \end{aligned}
\end{equation*}
其中 $\mathcal{A}(X) = \sum_{(i,j)\in \Omega} X_{ij}$ 是一个线性算子, $b=\sum_{(i,j)\in \Omega} M_{ij}$. 关于核范数的邻近算子可以用奇异值分解来计算:
\begin{equation*}
    \text{prox}_{t\|\cdot\|_*}(X) = U \text{diag}(\sigma_i - t)_+ V^T
\end{equation*}
其中 $X = U \text{diag}(\sigma_i) V^T$ 是 $X$ 的奇异值分解, $(\cdot)_+$ 表示将小于 0 的元素置为 0.

\subsection{交替方向乘子法}

对于低秩矩阵恢复问题, 我们可以使用交替方向乘子法进行求解. 首先, 我们引入拆分:
\begin{equation*}
    \begin{aligned}
        \min_{X, Y}\quad &\sum_{(i,j)\in \Omega} (X_{ij} - M_{ij})^2 + \mu \|Y\|_*\\
        \text{s.t.}\quad & X = Y
    \end{aligned}
\end{equation*}
然后构造增广拉格朗日函数:
\begin{equation*}
    L_\rho(X, Y; \lambda) = \sum_{(i,j)\in \Omega} (X_{ij} - M_{ij})^2 + \mu \|Y\|_* + \left<\lambda, X - Y\right> + \frac{\rho}{2}\|X - Y\|_2^2
\end{equation*}
使用交替方向乘子法求解. 其迭代格式如下:
\begin{align*}
    X_{k+1} &= \arg \min_{X} L_\rho(X, Y_k; \lambda_k)\\
    & = \arg \min_{X} \sum_{(i,j)\in \Omega} (X_{ij} - M_{ij})^2 + \left<\lambda_k, X\right> + \frac{\rho}{2}\|X - Y_k\|_2^2\\
    Y_{k+1} &= \arg \min_{Y} L_\rho(X_{k+1}, Y; \lambda_k)\\
    & = \arg \min_{Y} \mu \|Y\|_* + \left<\lambda_k, -Y\right> + \frac{\rho}{2}\|X_{k+1} - Y\|_2^2\\
    & =\text{prox}_{\mu\|\cdot\|_*}(X_{k+1} + \frac{\lambda_k}{\rho})\\
    \lambda_{k+1} &= \lambda_k + \tau(X_{k+1} - Y_{k+1})
\end{align*}

\section{数值结果}

\subsection{LASSO 问题}

在 LASSO 问题中, 矩阵 $A$ 是一个高斯随机的 $512\times 1024$ 的矩阵, 真解 $u$ 是一个 $1024$ 维的稀疏向量, 稀疏度为 $0.1$, $b=Au$, $\mu=0.01$, 各算法的求解时间, 迭代次数, 目标函数值, 求得的解与 $u$ 的差的二范数和解的稀疏度如下表所示:
\begin{table}[h]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        算法 & 求解时间 (s) & 迭代次数 & 目标函数值 & $\|x - u\|_2$ & 稀疏度\\
        \midrule
        Mosek & 0.37 & 8 & 0.923497 & 4.6127e-06 & 0.0996 \\
        Gurobi & 1.11 & 14 & 0.923486 & 3.5969e-04 & 0.1660 \\
        Prox GD & 0.051 & 269 & 0.923497 & 2.7640e-04 & 0.0996 \\
        ADMM Primal & 0.116 & 616 & 0.923507 & 3.7449e-04 & 0.1064 \\
        ADMM Dual & 0.045 & 112 & 0.923816 & 9.7095e-04 & 0.0986 \\
        PDHG & 0.141 & 240 & 0.923504 & 3.2718e-04 & 0.1055 \\
        \bottomrule
    \end{tabular}
    \caption{LASSO 问题的求解结果}
    \label{tab:lasso}
\end{table}

由表 \ref{tab:lasso} 可以看出, 商业求解器 Mosek 和 Gurobi 在求解时使用的内点法虽然迭代步数少, 但是每一步迭代的计算量较大, 因此求解时间较长. 而其他算法利用了问题的结构, 虽然迭代步数较多, 但是每一步的计算量小, 因此求解时间较短. 此外, 同样使用 ADMM 算法, 直接求解原问题的迭代步数较多, 同时目标函数值和误差也更小. 与之相反, 求解对偶问题的迭代步数较少, 但是目标函数值和误差较大. PDHG 算法虽然迭代步数和每一步的计算量都较少, 但是由于确定迭代步长时需要计算矩阵的二范数, 因此求解时间较长.

\begin{table}[h]
    \centering
    \begin{tabular}{cccc}
        \toprule
        算法 & CPU 求解时间 (s) & GPU 求解时间 (s) & 加速比 \\
        \midrule
        Prox GD & 0.051 & 0.024 & 2.1 \\
        ADMM Primal & 0.116 & 0.073 & 1.6 \\
        ADMM Dual & 0.045 & 0.011 & 4.0 \\
        PDHG & 0.141 & 0.038 & 3.7 \\
        \bottomrule
    \end{tabular}
    \caption{GPU 加速与 CPU 求解的时间对比}
    \label{tab:gpu_cpu_comparison}
\end{table}

表 \ref{tab:gpu_cpu_comparison} 中对比了使用 GPU 加速的算法与使用 CPU 求解的算法的求解时间, 和加速比. 从表中的结果可以看出, 在使用 GPU 加速后, 所有算法的求解时间都有所减少, 其中 ADMM Dual 算法和 PDHG 算法的加速加速效果最明显. 但是, 与之前的理论分析结果不同, ADMM Primal 和 ADMM Dual 算法同样使用了矩阵求逆, 但是 ADMM Dual 算法的加速效果显著好于 ADMM Primal 算法.

\subsection{低秩矩阵恢复问题}

在低秩矩阵恢复问题中, 我们取矩阵 $M$ 为一个 $40\times40$ 的矩阵, 采样比例分取 $0.3$ 和 $0.5$, $\mu$ 取 $0.1$, $0.01$, $0.001$, 分别记录两种算法的求解时间, 迭代次数, 目标函数值, 求得的解与 $M$ 的差的 Frobenius 范数和解的秩. 结果如下表所示:

\begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{cccccccc}
            \toprule
            算法 & 采样率 & $\mu$ & 求解时间 (s) & 迭代次数 & 目标函数值 & Frobenius 范数 & 秩\\
            \midrule
            Prox GD & 0.3 & 0.1 & 0.978 & 3799 & 12.057304 & 1.6419e+01 & 14 \\
            Prox GD & 0.3 & 0.01 & 1.049 & 3429 & 1.210707 & 1.6570e+01 & 15 \\
            Prox GD & 0.3 & 0.001 & 0.830 & 2723 & 0.121319 & 2.0295e+01 & 18 \\
            Prox GD & 0.5 & 0.1 & 0.179 & 570 & 12.362322 & 3.0684e-01 & 3 \\
            Prox GD & 0.5 & 0.01 & 0.204 & 764 & 1.238130 & 3.1128e-02 & 3 \\
            Prox GD & 0.5 & 0.001 & 0.322 & 1068 & 0.123832 & 6.1531e-03 & 5 \\
            ADMM & 0.3 & 0.1 & 1.041 & 3493 & 12.057298 & 1.6351e+01 & 14 \\
            ADMM & 0.3 & 0.01 & 0.985 & 3588 & 1.210679 & 1.6288e+01 & 15 \\
            ADMM & 0.3 & 0.001 & 0.940 & 3602 & 0.121118 & 1.6289e+01 & 15 \\
            ADMM & 0.5 & 0.1 & 0.128 & 477 & 12.362322 & 3.0659e-01 & 3 \\
            ADMM & 0.5 & 0.01 & 0.216 & 798 & 1.238130 & 3.0839e-02 & 3 \\
            ADMM & 0.5 & 0.001 & 1.164 & 3942 & 0.123832 & 3.0857e-03 & 3 \\
            \bottomrule
        \end{tabular}
    }
    \caption{低秩矩阵恢复问题的求解结果}
    \label{tab:low_rank_recovery}
\end{table}

从表 \ref{tab:low_rank_recovery} 中可以看出, 在我们的算例中, 采样率为 $0.3$ 时, 无论 $\mu$ 取何值, 该优化问题的解并不收敛到真实值, 求得的最优解的秩也远比真实值高, 两种算法也需要很多迭代步才能收敛. 但是, 在采样率为 $0.5$ 时, 该优化问题的解收敛到真实值, 求得的最优解的秩也与真实值相同, 两种算法的迭代步数也大大减少.

对比 ADMM 和 Prox GD 算法, 从采样率为 $0.5$, $\mu=0.001$ 的算例可以看出, ADMM 算法能在 $\mu$ 较小时收敛到真实值, 但是同时也需要更多的迭代步数, 而 Prox GD 算法在 $\mu$ 较小时会因达到收敛条件而提前停止, 导致并未很好的逼近真实值.

\end{document}
